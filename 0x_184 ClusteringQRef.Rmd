---
title: "Clustering in R Quick Reference"
author: "Mark Niemann-Ross"
date: "`r Sys.Date()`"

output:
  html_document:
    df_print: paged
    toc: yes
  github_document:
    toc: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(cluster)

```


# About
## About This Document

This document provides brief explanations of the clustering functions available in Base R and the cluster package. 

## Related Material
[The cluster package](https://cran.r-project.org/web/packages/cluster/index.html) is recommended for advanced operations. The cluster package is based on [Finding Groups in Data](https://onlinelibrary.wiley.com/doi/book/10.1002/9780470316801) 

An exhaustive discussion of clustering can be found at [Machine Learning and AI foundations:clustering and association](https://linkedin-learning.pxf.io/lilclustering)

This Quick Reference is supplemental to courses on [LinkedIn Learning](https://www.linkedin.com/learning/r-for-data-science-lunchbreak-lessons/). A quick reference to [matrix math functions in R is here](http://niemannross.com/link/rmatrixqref). A quick reference to [plotting functions in R is here](http://niemannross.com/link/rplotqref). An index to all R functions covered at LinkedIn Learning is found [here](http://niemannross.com/link/rindex). The latest version of this quick reference is found [here](http://niemannross.com/link/rclusterqref). The source to this document is found on [github/mnr](https://github.com/mnr/R-for-Data-Science-Lunchbreak-Lessons). This document is available as Free Software under the terms of the [Free Software Foundationâ€™s](http://www.gnu.org/) [GNU General Public License](https://www.r-project.org/COPYING).

## About Mark Niemann-Ross
[Mark](https://niemannross.com/) is an author for LinkedIn Learning and writes Science Fiction.

# Partitioning methods
Divide dataset into k clusters. (k is defined by researcher)

## *kmeans* : Partitioning around centroids

```{r, echo=TRUE}

# kmeans clusters data together, based on average distance from a centroid
# kmeans iterates through possible solutions to identify the best result

kmeans(quakes, 3)

# lots of information in the print from kmeans
# cluster sizes
# cl$centers - cluster means - shows the centroid for each cluster
# cl$cluster - clustering vector - shows which data points belong to which cluster
# cl$withinss - within cluster sum of squares - small SofS = more compact cluster


cl <- kmeans(quakes, 3)
plot(quakes, col = cl$cluster)
# what are we looking at.

#simplify
simpleQuakes <- quakes[,c("long","lat")]
cl <- kmeans(simpleQuakes, 4)
plot(simpleQuakes, col = cl$cluster)



```




## *pam* : Partitioning Around Medoids

```{r, echo=TRUE}
simpleQuakes <- quakes[,c("long","lat")]
mypam <- pam(simpleQuakes,2) # assuming 2 medoids

# components of mypam - see pam.object
# for example...
mypam$medoids # coordinates of medoids
mypam$id.med # indices of medoids (i.e. simpleQuakes[mypam$id.med,])

# mypam$clustering # which points belong to which medoid
# same as... pam(simpleQuakes, 4, cluster.only = TRUE)


# plotting PAM
#plot(mypam)
#plot(mypam, ask = TRUE) # menu for plots

#map("worldHires","Fiji", xlim=c(160,190), ylim =c(-40,-15))
plot(mypam, which.plots = 1)

```

## *clara* : Clustering Large Applications
Clara is identical to pam() except subsamples the data before determining medoids.

```{r, echo=TRUE}
myclara <- clara(quakes,2)
plot(myclara, which.plots = 1) #graph
plot(myclara, which.plots = 2) #silhoutte

```


## *fanny* : Fuzzy Analysis Clustering
```{r, echo=TRUE}
# downsample quakes for demonstration. from 500 to 25
simpleQuakes <- quakes[sample(nrow(quakes),25),c("long","lat")]

# create a fuzzy cluster with fanny
fuzCluster <- fanny(x = simpleQuakes, k = 3) 
fuzCluster

# this produces 4 parts: object, membership, fuzzyness, other components of object
fuzCluster$membership # percentage likelihood this point is part of this cluster. 
fuzCluster$coeff # A low value of Dunn's coefficient indicates a very fuzzy clustering, whereas a value close to 1 indicates a near-crisp clustering
fuzCluster$clustering # closest hard/crisp cluster

```



# Hierarchical methods
Start with cluster of one item, then build larger clusters (or the opposite)

## *agnes* : Agglomerative Nesting

```{r, echo=TRUE}
agnes(agriculture)
```


```{r, echo=TRUE}
plot(agnes(agriculture), which.plots = 2)
```

## *hclust* : Hierarchical Clustering

```{r, echo=TRUE}
# builds a dendogram

# what is a distance matrix? (distance between points)
sampleQuakes <- quakes[sample(1:length(quakes$lat), 5),] # demonstrate a small set
sampleQuakes # here's the data
dist(sampleQuakes) # here's the distance matrix

# now here's a hierarchical cluster
hclust(dist(quakes)) # returns an hclust object

plot(hclust(dist(quakes))) # overplot
plot(hclust(dist(sampleQuakes))) # more approachable
# starts with one cluster per element
# Progressing up dendrogram produces larger clusters until one cluster at top
# Labels are row numbers. 
plot(hclust(dist(sampleQuakes)),
     labels = c(LETTERS[1:length(sampleQuakes)])) # labeled points

```

## *diana* : DIvisive ANAlysis Clustering
## *mona* : MONothetic Analysis Clustering of Binary Variables

# Dissimilarity

## *daisy* : Dissimilarity Matrix Calculation

daisy creates a dissimilarity matrix
```{r, echo=TRUE}
daisy(head(agriculture))
```





